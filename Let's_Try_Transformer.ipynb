{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEh5oQvofKME"
   },
   "outputs": [],
   "source": [
    "# Google Colabでドライブのデータを使う\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NShC5tJrgLCC"
   },
   "outputs": [],
   "source": [
    "# --- 組み込みモジュール ---\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "# --- パスの設定（ローカルモジュールを読み込むための設定） ---\n",
    "sys.path.append('/content/drive/Shareddrives/MuraolabDocument/技術/機械学習/Transformer/')\n",
    "\n",
    "# --- ローカルモジュール ---\n",
    "import Transformer_Encoder as te\n",
    "\n",
    "# --- サードパーティモジュール ---\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTYyvkuLopZv"
   },
   "outputs": [],
   "source": [
    "# === 1. データの読み込み ===\n",
    "def load_signals(file_paths):\n",
    "    \"\"\"\n",
    "    複数の信号ファイルを読み込み、各サンプルごとに信号をスタックして3次元配列にする\n",
    "    \"\"\"\n",
    "    signals = [np.loadtxt(fp) for fp in file_paths]  # 各配列の shape: (サンプル数, 128)\n",
    "    return np.stack(signals, axis=-1)  # 結果の shape: (サンプル数, 128, 信号数)\n",
    "\n",
    "# データセットのディレクトリ\n",
    "data_dir = \"/content/drive/Shareddrives/MuraolabDocument/技術/機械学習/Transformer/UCI HAR Dataset\"\n",
    "\n",
    "# --- 訓練データ ---\n",
    "train_signals_dir = f'{data_dir}/train/Inertial Signals/'\n",
    "signal_files_train = [\n",
    "    'body_acc_x_train.txt',\n",
    "    'body_acc_y_train.txt',\n",
    "    'body_acc_z_train.txt',\n",
    "    'body_gyro_x_train.txt',\n",
    "    'body_gyro_y_train.txt',\n",
    "    'body_gyro_z_train.txt',\n",
    "    'total_acc_x_train.txt',\n",
    "    'total_acc_y_train.txt',\n",
    "    'total_acc_z_train.txt'\n",
    "]\n",
    "train_file_paths = [train_signals_dir + fname for fname in signal_files_train]\n",
    "X_train = load_signals(train_file_paths)  # shape: (7352, 128, 9)\n",
    "\n",
    "# --- テストデータ ---\n",
    "test_signals_dir = f'{data_dir}/test/Inertial Signals/'\n",
    "signal_files_test = [fname.replace('_train', '_test') for fname in signal_files_train]\n",
    "test_file_paths = [test_signals_dir + fname for fname in signal_files_test]\n",
    "X_test = load_signals(test_file_paths)  # shape: (2947, 128, 9)\n",
    "\n",
    "# --- ラベルの読み込み ---\n",
    "y_train = np.loadtxt(f'{data_dir}/train/y_train.txt')\n",
    "y_test = np.loadtxt(f'{data_dir}/test/y_test.txt')\n",
    "\n",
    "# ラベルを 0〜5 に変換（もともとは 1〜6）\n",
    "y_train = y_train.astype(np.int32) - 1\n",
    "y_test = y_test.astype(np.int32) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVqZXrVkopcA"
   },
   "outputs": [],
   "source": [
    "# === 2. 標準化 ===\n",
    "# UCI HARデータセットはすでに標準化されているため、以下の標準化処理はデフォルトではコメントアウトしている。\n",
    "# 別のデータセットを使用する場合は、コメントアウトを外して利用できる。\n",
    "\n",
    "\"\"\"\n",
    "n_train, timesteps, n_features = X_train.shape\n",
    "scaler = StandardScaler()\n",
    "# 各チャネルごとに2次元に変形して標準化\n",
    "X_train_reshaped = X_train.reshape(-1, n_features)  # shape: (n_train * timesteps, n_features)\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped).reshape(n_train, timesteps, n_features)\n",
    "\n",
    "n_test = X_test.shape[0]\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, n_features)).reshape(n_test, timesteps, n_features)\n",
    "\"\"\"\n",
    "X_train_scaled = X_train\n",
    "X_test_scaled = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fa7oMiz-opeU"
   },
   "outputs": [],
   "source": [
    "# === 3. 最大データ長に合わせてパディング ===\n",
    "# UCI HARデータセットはすでに長さが統一されているため意味なし\n",
    "PAD = 0.0\n",
    "max_length_train = max(len(seq) for seq in X_train_scaled)\n",
    "max_length_test = max(len(seq) for seq in X_test_scaled)\n",
    "max_length = max(max_length_train, max_length_test)\n",
    "\n",
    "#シーケンスのパディング\n",
    "X_train_padded = pad_sequences(X_train_scaled, maxlen=max_length, padding='post', value=PAD, dtype='float32')\n",
    "X_test_padded = pad_sequences(X_test_scaled, maxlen=max_length, padding='post', value=PAD, dtype='float32')\n",
    "\n",
    "#パディング後のデータに NaN が含まれていないか確認\n",
    "print('NaN in X_train_padded:', np.isnan(X_train_padded).any())\n",
    "print('NaN in X_test_padded:', np.isnan(X_test_padded).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzOML1A1opgZ"
   },
   "outputs": [],
   "source": [
    "# === 4. 訓練データの中から検証データを分割 ===\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_padded, y_train, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKa4Nw_ngq0-"
   },
   "outputs": [],
   "source": [
    "# === 5. Transformer モデルの構築・コンパイル・学習 ===\n",
    "config = te.TransformerConfig(\n",
    "    max_length=max_length,               # シーケンスの長さ\n",
    "    d_model=X_train_final.shape[2],      # 特徴量の次元数（ここでは 9）\n",
    "    key_dim=16,                           # 各アテンションヘッドの次元数\n",
    "    num_heads=8,                         # アテンションヘッドの数\n",
    "    ff_dim=X_train_final.shape[2] * 4,   # FeedForwardネットワークの中間次元数（9×4=36）\n",
    "    num_transformer_blocks=1,            # トランスフォーマーブロックの数\n",
    "    dropout=0.1,                         # ドロップアウト率\n",
    "    l2_lambda=1e-4,                      # L2正則化係数\n",
    "    pad=0.0,                             # パディングの値\n",
    "    pooling='average',                   # プーリングの方法 ('average' または 'max')\n",
    "    task='multiclass',                   # タスクの種類（今回は多クラス分類）\n",
    "    num_classes=6                        # クラス数（6クラス）\n",
    ")\n",
    "\n",
    "# モデルの構築\n",
    "model = te.build_transformer_model(config)\n",
    "\n",
    "# モデルのコンパイル（多クラス分類用損失関数）\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# モデル概要の表示\n",
    "model.summary()\n",
    "\n",
    "# Early Stoppingの実装\n",
    "# early_stop = EarlyStopping(\n",
    "#     monitor='val_loss',      # 検証損失を監視する\n",
    "#     patience=5,              # 5エポック改善が見られなければ学習を停止する\n",
    "#     restore_best_weights=True  # 学習中の最良の重みを復元する\n",
    "# )\n",
    "\n",
    "# モデルの学習（検証データとして X_val, y_val を利用）\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val, y_val)\n",
    "    #validation_split=0.2,     #データの何割を検証用に使用するか，本コードでは手順４番で明示的に検証用データを切り出しているので不要\n",
    "    #callbacks=[early_stop]     ＃設定したEarly Stoppingに沿って途中で学習を停止する\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cF8kFICltyf8"
   },
   "outputs": [],
   "source": [
    "# === 5. テストデータでモデルの性能を評価 ===\n",
    "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRl6e_gnYkek"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 学習曲線のプロット\n",
    "# -----------------------------\n",
    "# lossの推移をプロット\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# accuracy の推移をプロット\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxln9ZWC_EV4"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 混同行列の出力\n",
    "# -----------------------------\n",
    "# テストデータで予測\n",
    "y_pred_probs = model.predict(X_test_padded)\n",
    "# sparse_categorical_crossentropy を使用している場合、予測結果は各クラスの確率となるため、\n",
    "# argmaxを用いてクラスラベルを取得する\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# 混同行列の計算\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbGP3F0c9ueA"
   },
   "outputs": [],
   "source": [
    "# 予測済みラベルは、既に y_pred に格納されている前提\n",
    "target_names = [\"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\", \"SITTING\", \"STANDING\", \"LAYING\"]\n",
    "report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frnlHGBEVuCQ"
   },
   "outputs": [],
   "source": [
    "# scaler を保存\n",
    "with open(f'/content/drive/Shareddrives/MuraolabDocument/技術/機械学習/Transformer/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# 保存したscalerを読み込む\n",
    "with open('/content/drive/Shareddrives/MuraolabDocument/技術/機械学習/Transformer/scaler.pkl', \"rb\") as f:\n",
    "    spring_final_index_dict = pickle.load(f)\n",
    "\n",
    "# modelを保存する\n",
    "model.save(f'/content/drive/Shareddrives/MuraolabDocument/技術/機械学習/Transformer/model.keras')\n",
    "# 保存したmodelを読み込む\n",
    "model = load_model('/content/drive/Shareddrives/MuraolabDocument/技術/機械学習/Transformer/model.keras')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMeA8cKF4g3s7fj95cu8Ywq",
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
